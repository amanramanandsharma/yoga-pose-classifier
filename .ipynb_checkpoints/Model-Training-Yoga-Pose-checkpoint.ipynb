{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53836cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from movenet import Movenet\n",
    "import wget\n",
    "import csv\n",
    "import tqdm \n",
    "from data import BodyPart\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "movenet = Movenet('movenet_thunder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52609f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(input_tensor, inference_count=3):\n",
    "    movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
    "    \n",
    "    for _ in range(inference_count - 1):\n",
    "        detection = movenet.detect(input_tensor.numpy(), \n",
    "                                reset_crop_region=False)\n",
    "    \n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd56d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "#     this class preprocess pose samples, it predicts keypoints on the images \n",
    "#     and save those keypoints in a csv file for the later use in the classification task \n",
    "\n",
    "        def __init__(self, images_in_folder,\n",
    "                    csvs_out_path):\n",
    "            self._images_in_folder = images_in_folder\n",
    "            self._csvs_out_path = csvs_out_path\n",
    "            self._csvs_out_folder_per_class = 'csv_per_pose'\n",
    "            self._message = []\n",
    "            \n",
    "            if(self._csvs_out_folder_per_class not in os.listdir()):\n",
    "                os.makedirs(self._csvs_out_folder_per_class)\n",
    "            \n",
    "#             get list of pose classes\n",
    "            self._pose_class_names = sorted(\n",
    "                [n for n in os.listdir(images_in_folder)]\n",
    "            )\n",
    "    \n",
    "\n",
    "        \n",
    "        def process(self, detection_threshold=0.1):\n",
    "#             Preprocess the images in the given folder\n",
    "            for pose_class_name in self._pose_class_names:\n",
    "#                 paths for pose class\n",
    "                images_in_folder = os.path.join(self._images_in_folder, pose_class_name)\n",
    "                csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
    "                                               pose_class_name + '.csv'\n",
    "                                           )\n",
    "#               Detect landmarks in each images and write it to the csv files\n",
    "                with open(csv_out_path, 'w') as csv_out_file:\n",
    "                    csv_out_writer = csv.writer(csv_out_file,\n",
    "                                                delimiter=',',\n",
    "                                                quoting=csv.QUOTE_MINIMAL\n",
    "                                               )\n",
    "    #             get the list of images\n",
    "                    image_names = sorted(\n",
    "                        [n for n in os.listdir(images_in_folder)]\n",
    "                    )\n",
    "                    valid_image_count = 0\n",
    "                    # Detect pose landmarks in each image\n",
    "                    for image_name in tqdm.tqdm(image_names):\n",
    "                        image_path = os.path.join(images_in_folder, image_name)\n",
    "                        \n",
    "                        try:\n",
    "                            image = tf.io.read_file(image_path)\n",
    "                            image = tf.io.decode_jpeg(image)\n",
    "                        except:\n",
    "                            self._message.append('Skipped' + image_path + ' Invalid image')\n",
    "                            continue\n",
    "                        \n",
    "                        # skip images that is not RGB\n",
    "                        if image.shape[2] != 3:\n",
    "                            self.message.append('Skipped' + image_path + ' Image is not in RGB')\n",
    "                            continue\n",
    "                        \n",
    "                        person = detect(image)\n",
    "                        \n",
    "                        # Save landmarks if all landmarks above than the threshold\n",
    "                        min_landmark_score = min([keypoint.score for keypoint in person.keypoints])\n",
    "                        should_keep_image = min_landmark_score >= detection_threshold\n",
    "                        if not should_keep_image:\n",
    "                            self._message.append('Skipped' + image_path + 'Keypoints score are below than threshold')\n",
    "                            continue\n",
    "                            \n",
    "                        valid_image_count += 1\n",
    "                        \n",
    "                        # Get landmarks and scale it to the same size as the input image\n",
    "                        pose_landmarks = np.array(\n",
    "                              [[keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score]\n",
    "                                for keypoint in person.keypoints],\n",
    "                                  dtype=np.float32)\n",
    "                        \n",
    "                        # writing the landmark coordinates to its csv files\n",
    "                        coord = pose_landmarks.flatten().astype(np.str).tolist()\n",
    "                        csv_out_writer.writerow([image_name] + coord)\n",
    "                        \n",
    "            print(self._message)\n",
    "\n",
    "            # combine all per-csv class CSVs into a sigle csv file\n",
    "            all_landmarks_df = self.all_landmarks_as_dataframe()\n",
    "            all_landmarks_df.to_csv(self._csvs_out_path, index=False)\n",
    "\n",
    "        def class_names(self):\n",
    "            return self.pose_class_names\n",
    "        \n",
    "        def all_landmarks_as_dataframe(self):\n",
    "            # Merging all csv for each class into a single csv file\n",
    "            total_df = None\n",
    "            for class_index, class_name in enumerate(self._pose_class_names):\n",
    "                csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
    "                                               class_name + '.csv'\n",
    "                                           )\n",
    "                per_class_df = pd.read_csv(csv_out_path, header=None)\n",
    "                \n",
    "                # Add the labels\n",
    "                per_class_df['class_no'] = [class_index]*len(per_class_df)\n",
    "                per_class_df['class_name'] = [class_name]*len(per_class_df)\n",
    "                \n",
    "                # Append the folder name to the filename first column\n",
    "                per_class_df[per_class_df.columns[0]] = class_name + '/' +  per_class_df[per_class_df.columns[0]]\n",
    "                \n",
    "                if total_df is None:\n",
    "                    total_df = per_class_df\n",
    "                else:\n",
    "                    total_df = pd.concat([total_df, per_class_df], axis=0)\n",
    "            \n",
    "            list_name = [[bodypart.name + '_x', bodypart.name + '_y', \n",
    "                  bodypart.name + '_score'] for bodypart in BodyPart]\n",
    "            \n",
    "            header_name = []\n",
    "            for columns_name in list_name:\n",
    "                header_name += columns_name\n",
    "            header_name = ['filename'] + header_name\n",
    "            header_map = { total_df.columns[i]: header_name[i]\n",
    "                             for i in range(len(header_name))\n",
    "                         }\n",
    "            \n",
    "            total_df.rename(header_map, axis=1, inplace=True)\n",
    "            \n",
    "            return total_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess training data\n",
    "images_in_folder = os.path.join('yoga_poses', 'train')\n",
    "csvs_out_path = 'train_data.csv'\n",
    "train_preprocessor = Preprocessor(\n",
    "    images_in_folder,\n",
    "    csvs_out_path\n",
    ")\n",
    "train_preprocessor.process()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13156dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing testing data\n",
    "images_in_folder = os.path.join('yoga_poses', 'test')\n",
    "csvs_out_path = 'test_data.csv'\n",
    "test_preprocessor = Preprocessor(\n",
    "    images_in_folder,\n",
    "    csvs_out_path\n",
    ")\n",
    "test_preprocessor.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ae814",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data import BodyPart \n",
    "import tensorflow as tf\n",
    "# import tensorflowjs as tfjs\n",
    "\n",
    "tfjs_model_dir = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6dd555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading final csv file\n",
    "def load_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.drop(['filename'],axis=1, inplace=True)\n",
    "    classes = df.pop('class_name').unique()\n",
    "    y = df.pop('class_no')\n",
    "    \n",
    "    X = df.astype('float64')\n",
    "    y = keras.utils.to_categorical(y)\n",
    "    \n",
    "    return X, y, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8bd41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
    "    \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n",
    "    left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
    "    right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
    "    center = left * 0.5 + right * 0.5\n",
    "    return center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24acecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
    "    \"\"\"Calculates pose size.\n",
    "\n",
    "    It is the maximum of two values:\n",
    "    * Torso size multiplied by `torso_size_multiplier`\n",
    "    * Maximum distance from pose center to any pose landmark\n",
    "    \"\"\"\n",
    "    # Hips center\n",
    "    hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "    # Shoulders center\n",
    "    shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n",
    "                                      BodyPart.RIGHT_SHOULDER)\n",
    "\n",
    "    # Torso size as the minimum body size\n",
    "    torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
    "    # Pose center\n",
    "    pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                     BodyPart.RIGHT_HIP)\n",
    "    pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to\n",
    "    # perform substraction\n",
    "    pose_center_new = tf.broadcast_to(pose_center_new,\n",
    "                                    [tf.size(landmarks) // (17*2), 17, 2])\n",
    "\n",
    "    # Dist to pose center\n",
    "    d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
    "                name=\"dist_to_pose_center\")\n",
    "    # Max dist to pose center\n",
    "    max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
    "\n",
    "    # Normalize scale\n",
    "    pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
    "    return pose_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pose_landmarks(landmarks):\n",
    "    \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
    "    scaling it to a constant pose size.\n",
    "  \"\"\"\n",
    "  # Move landmarks so that the pose center becomes (0,0)\n",
    "    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "    pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to perform\n",
    "    # substraction\n",
    "    pose_center = tf.broadcast_to(pose_center, \n",
    "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
    "    landmarks = landmarks - pose_center\n",
    "\n",
    "    # Scale the landmarks to a constant pose size\n",
    "    pose_size = get_pose_size(landmarks)\n",
    "    landmarks /= pose_size\n",
    "    return landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba8ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_to_embedding(landmarks_and_scores):\n",
    "    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
    "    # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "    reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "    # Normalize landmarks 2D\n",
    "    landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])\n",
    "    # Flatten the normalized landmark coordinates into a vector\n",
    "    embedding = keras.layers.Flatten()(landmarks)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train):\n",
    "    processed_X_train = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        embedding = landmarks_to_embedding(tf.reshape(tf.convert_to_tensor(X_train.iloc[i]), (1, 51)))\n",
    "        processed_X_train.append(tf.reshape(embedding, (34)))\n",
    "    return tf.convert_to_tensor(processed_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a37116",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, class_names = load_csv('train_data.csv')\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15)\n",
    "X_test, y_test, _ = load_csv('test_data.csv')\n",
    "\n",
    "\n",
    "processed_X_train = preprocess_data(X_train)\n",
    "processed_X_val =  preprocess_data(X_val)\n",
    "processed_X_test = preprocess_data(X_test)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(34))\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(inputs)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca95777",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a checkpoint callback to store the checkpoint that has the highest\n",
    "# validation accuracy.\n",
    "checkpoint_path = \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1697014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print('--------------TRAINING----------------')\n",
    "history = model.fit(processed_X_train, y_train,\n",
    "                    epochs=2000,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(processed_X_val, y_val),\n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b64f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('-----------------EVAUATION----------------')\n",
    "loss, accuracy = model.evaluate(processed_X_test, y_test)\n",
    "print('LOSS: ', loss)\n",
    "print(\"ACCURACY: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e522838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tfjs.converters.save_keras_model(model, tfjs_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5db20",
   "metadata": {},
   "source": [
    "# If you directly want to run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ba5c80",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.5.5.62)\n",
      "Requirement already satisfied: mediapipe in c:\\programdata\\anaconda3\\lib\\site-packages (0.8.9.1)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.43.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (21.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (3.4.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (4.5.5.62)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow opencv-python mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b030ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from movenet import Movenet\n",
    "from data import BodyPart \n",
    "\n",
    "movenet = Movenet('movenet_thunder')\n",
    "\n",
    "model = keras.models.load_model('yoga_trained_model')\n",
    "\n",
    "def detect(input_tensor, inference_count=3):\n",
    "    movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
    "    \n",
    "    for _ in range(inference_count - 1):\n",
    "        detection = movenet.detect(input_tensor.numpy(), \n",
    "                                reset_crop_region=False)\n",
    "    \n",
    "    return detection\n",
    "\n",
    "def landmarks_to_embedding(landmarks_and_scores):\n",
    "    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
    "    # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "    reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "    # Normalize landmarks 2D\n",
    "    landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])\n",
    "    # Flatten the normalized landmark coordinates into a vector\n",
    "    embedding = keras.layers.Flatten()(landmarks)\n",
    "    return embedding\n",
    "\n",
    "def normalize_pose_landmarks(landmarks):\n",
    "    \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
    "    scaling it to a constant pose size.\n",
    "  \"\"\"\n",
    "  # Move landmarks so that the pose center becomes (0,0)\n",
    "    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "    pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to perform\n",
    "    # substraction\n",
    "    pose_center = tf.broadcast_to(pose_center, \n",
    "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
    "    landmarks = landmarks - pose_center\n",
    "\n",
    "    # Scale the landmarks to a constant pose size\n",
    "    pose_size = get_pose_size(landmarks)\n",
    "    landmarks /= pose_size\n",
    "    return landmarks\n",
    "\n",
    "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
    "    \"\"\"Calculates pose size.\n",
    "\n",
    "    It is the maximum of two values:\n",
    "    * Torso size multiplied by `torso_size_multiplier`\n",
    "    * Maximum distance from pose center to any pose landmark\n",
    "    \"\"\"\n",
    "    # Hips center\n",
    "    hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "    # Shoulders center\n",
    "    shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n",
    "                                      BodyPart.RIGHT_SHOULDER)\n",
    "\n",
    "    # Torso size as the minimum body size\n",
    "    torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
    "    # Pose center\n",
    "    pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                     BodyPart.RIGHT_HIP)\n",
    "    pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to\n",
    "    # perform substraction\n",
    "    pose_center_new = tf.broadcast_to(pose_center_new,\n",
    "                                    [tf.size(landmarks) // (17*2), 17, 2])\n",
    "\n",
    "    # Dist to pose center\n",
    "    d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
    "                name=\"dist_to_pose_center\")\n",
    "    # Max dist to pose center\n",
    "    max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
    "\n",
    "    # Normalize scale\n",
    "    pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
    "    return pose_size\n",
    "\n",
    "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
    "    \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n",
    "    left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
    "    right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
    "    center = left * 0.5 + right * 0.5\n",
    "    return center\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "241e0e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions that we try to detect\n",
    "actions = np.array(['chair', 'cobra', 'dog','no_pose','shoudler_stand','traingle','tree','warrior'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "932c305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Setting up the Pose function.\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Initializing mediapipe drawing class, useful for annotation.\n",
    "mp_drawing = mp.solutions.drawing_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b843a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPose(image, pose, display=True):\n",
    "    '''\n",
    "    This function performs pose detection on an image.\n",
    "    Args:\n",
    "        image: The input image with a prominent person whose pose landmarks needs to be detected.\n",
    "        pose: The pose setup function required to perform the pose detection.\n",
    "        display: A boolean value that is if set to true the function displays the original input image, the resultant image, \n",
    "                 and the pose landmarks in 3D plot and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The input image with the detected pose landmarks drawn.\n",
    "        landmarks: A list of detected landmarks converted into their original scale.\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the input image.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Convert the image from BGR into RGB format.\n",
    "    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform the Pose Detection.\n",
    "    results = pose.process(imageRGB)\n",
    "    \n",
    "    # Retrieve the height and width of the input image.\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Initialize a list to store the detected landmarks.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Check if any landmarks are detected.\n",
    "    if results.pose_landmarks:\n",
    "    \n",
    "        # Draw Pose landmarks on the output image.\n",
    "        mp_drawing.draw_landmarks(image=output_image, landmark_list=results.pose_landmarks,\n",
    "                                  connections=mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "        # Iterate over the detected landmarks.\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            \n",
    "            # Append the landmark into the list.\n",
    "            landmarks.append((int(landmark.x * width), int(landmark.y * height),\n",
    "                                  (landmark.z * width)))\n",
    "    \n",
    "    # Check if the original input image and the resultant image are specified to be displayed.\n",
    "    if display:\n",
    "    \n",
    "        # Display the original input image and the resultant image.\n",
    "        plt.figure(figsize=[22,22])\n",
    "        plt.subplot(121);plt.imshow(image[:,:,::-1]);plt.title(\"Original Image\");plt.axis('off');\n",
    "        plt.subplot(122);plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "        \n",
    "        # Also Plot the Pose landmarks in 3D.\n",
    "        mp_drawing.plot_landmarks(results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the found landmarks.\n",
    "        return output_image, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a552ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# Create named window for resizing purposes\n",
    "cv2.namedWindow('Pose Detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "\n",
    "# Initialize the VideoCapture object to read from a video stored in the disk.\n",
    "#ideo = cv2.VideoCapture('media/running.mp4')\n",
    "\n",
    "# Set video camera size\n",
    "video.set(3,1280)\n",
    "video.set(4,960)\n",
    "\n",
    "\n",
    "# Iterate until the video is accessed successfully.\n",
    "while video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = video.read()\n",
    "    \n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the width and height of the frame\n",
    "    frame_height, frame_width, _ =  frame.shape\n",
    "    \n",
    "    # Resize the frame while keeping the aspect ratio.\n",
    "    frame = cv2.resize(frame, (int(frame_width * (640 / frame_height)), 640))\n",
    "    \n",
    "    # Perform Pose landmark detection.\n",
    "    frame2, results = detectPose(frame, pose, display=False)\n",
    "    img_name = \"opencv_frame.jpg\"\n",
    "    cv2.imwrite(img_name, frame)\n",
    "    \n",
    "    image_path = 'opencv_frame.jpg'\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "    \n",
    "    person = detect(image)\n",
    "\n",
    "    # Get landmarks and scale it to the same size as the input image\n",
    "    pose_landmarks = np.array(\n",
    "                                  [[keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score]\n",
    "                                    for keypoint in person.keypoints],\n",
    "                                      dtype=np.float32)\n",
    "                        \n",
    "    # writing the landmark coordinates to its csv files\n",
    "    coord = pose_landmarks.flatten().astype(np.float32).tolist()\n",
    "    embedding = landmarks_to_embedding(tf.reshape(tf.convert_to_tensor(coord), (1, 51)))\n",
    "    \n",
    "    # Perform the Pose Classification.\n",
    "    res = model.predict(embedding)\n",
    "    # Write the label on the output image.\n",
    "    label = actions[np.argmax(res)]\n",
    "    color = (0, 0, 255)\n",
    "    cv2.putText(frame2, label, (120,200), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 5, (0,255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "        \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Pose Detection', frame2)\n",
    "    \n",
    "    # Wait until a key is pressed.\n",
    "    # Retreive the ASCII code of the key pressed\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed.\n",
    "    if(k == 27):\n",
    "        \n",
    "        # Break the loop.\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object.\n",
    "video.release()\n",
    "\n",
    "# Close the windows.\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e75e132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release the VideoCapture object and close the windows.\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
